{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from albumentations.augmentations import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations.core.composition import Compose, OneOf\n",
    "\n",
    "# from train import train, validate\n",
    "# from source.network import UNetPP\n",
    "# from source.dataset import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_ids, img_dir, mask_dir, img_ext, mask_ext, transform=None):\n",
    "        # Initialize the dataset with image and mask file information\n",
    "        self.img_ids = img_ids\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.img_ext = img_ext\n",
    "        self.mask_ext = mask_ext\n",
    "        self.transform = transform  # Data augmentation or transformation\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a specific sample (image and mask) from the dataset\n",
    "\n",
    "        img_id = self.img_ids[idx]  # Get the image ID\n",
    "\n",
    "        # Load the image using OpenCV\n",
    "        img = cv2.imread(os.path.join(self.img_dir, img_id + self.img_ext))\n",
    "\n",
    "        mask = []  # Initialize a list to store mask(s)\n",
    "        \n",
    "        # Load the mask image as grayscale and add it to the list\n",
    "        mask.append(cv2.imread(os.path.join(self.mask_dir, img_id + self.mask_ext), cv2.IMREAD_GRAYSCALE)[..., None])\n",
    "        \n",
    "        # Stack the mask(s) along the depth dimension to form a multi-channel mask\n",
    "        mask = np.dstack(mask)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            # If data augmentation or transformation is provided, apply it to both image and mask\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Normalize and transpose the image and mask data for compatibility with PyTorch\n",
    "        img = img.astype('float32') / 255\n",
    "        img = img.transpose(2, 0, 1)  # Transpose image dimensions (channels-first)\n",
    "        mask = mask.astype('float32') / 255\n",
    "        mask = mask.transpose(2, 0, 1)  # Transpose mask dimensions\n",
    "\n",
    "        # Return the image, mask, and additional information as a dictionary\n",
    "        return img, mask, {'img_id': img_id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Define a VGG-style block with convolutional layers, ReLU activation, and batch normalization\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass for the VGG block\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class UNetPP(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3, deep_supervision=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Define VGG blocks at different stages\n",
    "        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n",
    "        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n",
    "        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n",
    "\n",
    "        # Define lateral connections and additional VGG blocks\n",
    "        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n",
    "\n",
    "        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "\n",
    "        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "\n",
    "        self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            # If deep supervision is enabled, define multiple final convolution layers\n",
    "            self.final1 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final2 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final3 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final4 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "        else:\n",
    "            # If deep supervision is not used, define a single final convolution layer\n",
    "            self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass for the UNet++\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_2)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            # If using deep supervision, return multiple output tensors\n",
    "            output1 = self.final1(x0_1)\n",
    "            output2 = self.final2(x0_2)\n",
    "            output3 = self.final3(x0_3)\n",
    "            output4 = self.final4(x0_4)\n",
    "            return [output1, output2, output3, output4]\n",
    "        else:\n",
    "            # If not using deep supervision, return a single output tensor\n",
    "            output = self.final(x0_4)\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the values to initial states\n",
    "        self.val = 0  # Current value\n",
    "        self.avg = 0  # Running average\n",
    "        self.sum = 0  # Sum of values\n",
    "        self.count = 0  # Number of values\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        # Update the average with a new value\n",
    "        self.val = val\n",
    "        self.sum += val * n  # Add the new value to the sum\n",
    "        self.count += n  # Increment the count\n",
    "        self.avg = self.sum / self.count  # Recalculate the average\n",
    "\n",
    "def iou_score(output, target):\n",
    "    smooth = 1e-5  # A small value to avoid division by zero\n",
    "\n",
    "    if torch.is_tensor(output):\n",
    "        # If the output is a PyTorch tensor, convert it to a NumPy array\n",
    "        output = torch.sigmoid(output).data.cpu().numpy()\n",
    "    if torch.is_tensor(target):\n",
    "        # If the target is a PyTorch tensor, convert it to a NumPy array\n",
    "        target = target.data.cpu().numpy()\n",
    "\n",
    "    # Convert output and target to binary masks (True/False)\n",
    "    output_ = output > 0.5\n",
    "    target_ = target > 0.5\n",
    "\n",
    "    # Calculate the intersection and union of the binary masks\n",
    "    intersection = (output_ & target_).sum()\n",
    "    union = (output_ | target_).sum()\n",
    "\n",
    "    # Calculate the Intersection over Union (IoU) score with smoothing\n",
    "    return (intersection + smooth) / (union + smooth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from source.utils import iou_score, AverageMeter\n",
    "from albumentations import Resize\n",
    "from albumentations.augmentations import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations.core.composition import Compose, OneOf\n",
    "from albumentations.augmentations.geometric.rotate import RandomRotate90\n",
    "from source.network import UNetPP\n",
    "from source.dataset import DataSet\n",
    "\n",
    "# Define a function for training the segmentation model\n",
    "def train(deep_sup, train_loader, model, criterion, optimizer):\n",
    "    # Initialize average meters to track loss and IoU\n",
    "    avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter()}\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Create a progress bar for training\n",
    "    pbar = tqdm(total=len(train_loader))\n",
    "\n",
    "    # Check if GPU is available, and if so, use it\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Iterate through the training data\n",
    "    for input, target, _ in train_loader:\n",
    "        # Move input and target to the GPU\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Compute model output\n",
    "        if deep_sup:\n",
    "            outputs = model(input)\n",
    "            loss = 0\n",
    "            for output in outputs:\n",
    "                loss += criterion(output, target)\n",
    "            loss /= len(outputs)\n",
    "            iou = iou_score(outputs[-1], target)\n",
    "        else:\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            iou = iou_score(output, target)\n",
    "\n",
    "        # Zero out gradients, backpropagate, and update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update average meters with loss and IoU\n",
    "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "        avg_meters['iou'].update(iou, input.size(0))\n",
    "\n",
    "        # Update the progress bar with the current loss and IoU\n",
    "        postfix = OrderedDict([('loss', avg_meters['loss'].avg), ('iou', avg_meters['iou'].avg)])\n",
    "        pbar.set_postfix(postfix)\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Close the progress bar\n",
    "    pbar.close()\n",
    "\n",
    "    # Return a dictionary with average loss and IoU\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg), ('iou', avg_meters['iou'].avg)])\n",
    "\n",
    "\n",
    "# Define a function for validating the segmentation model\n",
    "def validate(deep_sup, val_loader, model, criterion):\n",
    "    # Initialize average meters to track loss and IoU\n",
    "    avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter()}\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Check if GPU is available, and if so, use it\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Create a progress bar for validation\n",
    "        pbar = tqdm(total=len(val_loader))\n",
    "        \n",
    "        # Iterate through the validation data\n",
    "        for input, target, _ in val_loader:\n",
    "            # Move input and target to the GPU\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Compute model output\n",
    "            if deep_sup:\n",
    "                outputs = model(input)\n",
    "                loss = 0\n",
    "                for output in outputs:\n",
    "                    loss += criterion(output, target)\n",
    "                loss /= len(outputs)\n",
    "                iou = iou_score(outputs[-1], target)\n",
    "            else:\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "                iou = iou_score(output, target)\n",
    "\n",
    "            # Update average meters with loss and IoU\n",
    "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "            avg_meters['iou'].update(iou, input.size(0))\n",
    "\n",
    "            # Update the progress bar with the current loss and IoU\n",
    "            postfix = OrderedDict([('loss', avg_meters['loss'].avg), ('iou', avg_meters['iou'].avg)]\n",
    "            pbar.set_postfix(postfix)\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Close the progress bar\n",
    "        pbar.close()\n",
    "\n",
    "    # Return a dictionary with average loss and IoU for the validation dataset\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg), ('iou', avg_meters['iou'].avg)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from source.network import UNetPP\n",
    "from argparse import ArgumentParser\n",
    "from albumentations.augmentations import transforms\n",
    "from albumentations.core.composition import Compose\n",
    "\n",
    "# Define a set of image transformations for validation\n",
    "val_transform = Compose([\n",
    "    transforms.Resize(256, 256),  # Resize the image to 256x256 pixels\n",
    "    transforms.Normalize(),  # Normalize the image\n",
    "])\n",
    "\n",
    "# Define a function for loading and preprocessing an image\n",
    "def image_loader(image_name):\n",
    "    # Read the image using OpenCV\n",
    "    img = cv2.imread(image_name)\n",
    "    \n",
    "    # Apply the validation transformations to the image\n",
    "    img = val_transform(image=img)[\"image\"]\n",
    "    \n",
    "    # Convert the image to a NumPy array of float32 and normalize it\n",
    "    img = img.astype('float32') / 255\n",
    "    \n",
    "    # Transpose the image dimensions to match the PyTorch format (channels, height, width)\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the \"config.yaml\" file for reading\n",
    "with open(\"config.yaml\") as f:\n",
    "    # Load the YAML content into a Python dictionary\n",
    "    config = yaml.load(f)\n",
    "\n",
    "# Extract configuration values from the dictionary\n",
    "extn = config[\"extn\"]  # File extension (e.g., \".jpg\", \".png\")\n",
    "epochs = config[\"epochs\"]  # Number of training epochs\n",
    "log_path = config[\"log_path\"]  # Path for log files\n",
    "mask_path = config[\"mask_path\"]  # Path to the mask images\n",
    "image_path = config[\"image_path\"]  # Path to the input images\n",
    "model_path = config[\"model_path\"]  # Path to the saved model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ordered dictionary for logging information during training\n",
    "log = OrderedDict([\n",
    "    ('epoch', []),        # List to store epoch numbers\n",
    "    ('loss', []),         # List to store training loss values\n",
    "    ('iou', []),          # List to store training IoU (Intersection over Union) values\n",
    "    ('val_loss', []),     # List to store validation loss values\n",
    "    ('val_iou', []),      # List to store validation IoU values\n",
    "])\n",
    "\n",
    "# Initialize a variable to track the best IoU during training\n",
    "best_iou = 0\n",
    "\n",
    "# Initialize a trigger variable, which is used for some condition in the code\n",
    "trigger = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split images into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wildcard file extension pattern by appending the extension with '*'\n",
    "extn_ = f\"*{extn}\"\n",
    "\n",
    "# Use the 'glob' function to find all image files with the specified extension in the 'image_path' directory\n",
    "img_ids = glob(os.path.join(image_path, extn_))\n",
    "\n",
    "# Extract the base filenames (without extension) from the list of image file paths\n",
    "img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n",
    "\n",
    "# Split the list of image IDs into training and validation sets using a test size of 20%\n",
    "train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of data augmentation transformations for the training dataset\n",
    "train_transform = Compose([\n",
    "    transforms.RandomRotate90(),  # Randomly rotate the image by 90 degrees\n",
    "    transforms.Flip(),  # Randomly flip the image horizontally or vertically\n",
    "    OneOf([\n",
    "        transforms.HueSaturationValue(),  # Randomly change hue, saturation, and value\n",
    "        transforms.RandomBrightness(),  # Randomly adjust brightness\n",
    "        transforms.RandomContrast(),  # Randomly adjust contrast\n",
    "    ], p=1),  # Randomly select one of the color augmentation operations with equal probability (p=1)\n",
    "    transforms.Resize(256, 256),  # Resize the image to 256x256 pixels\n",
    "    transforms.Normalize(),  # Normalize the image\n",
    "])\n",
    "\n",
    "# Define a set of transformations for the validation dataset\n",
    "val_transform = Compose([\n",
    "    transforms.Resize(256, 256),  # Resize the image to 256x256 pixels\n",
    "    transforms.Normalize(),  # Normalize the image\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataset\n",
    "train_dataset = DataSet(\n",
    "    img_ids=train_img_ids,    # List of image IDs for training\n",
    "    img_dir=image_path,       # Directory containing the input images\n",
    "    mask_dir=mask_path,       # Directory containing the mask images\n",
    "    img_ext=extn,             # File extension for input images (e.g., \".jpg\", \".png\")\n",
    "    mask_ext=extn,            # File extension for mask images\n",
    "    transform=train_transform  # Data augmentation and preprocessing for training\n",
    ")\n",
    "\n",
    "# Create the validation dataset\n",
    "val_dataset = DataSet(\n",
    "    img_ids=val_img_ids,      # List of image IDs for validation\n",
    "    img_dir=image_path,       # Directory containing the input images\n",
    "    mask_dir=mask_path,       # Directory containing the mask images\n",
    "    img_ext=extn,             # File extension for input images\n",
    "    mask_ext=extn,            # File extension for mask images\n",
    "    transform=val_transform  # Data preprocessing for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and validation data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader for the training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,  # The training dataset\n",
    "    batch_size=16,    # Batch size for each iteration\n",
    "    shuffle=True,     # Shuffle the data during training (randomize the order)\n",
    "    drop_last=True    # Drop the last batch if its size is less than the specified batch size\n",
    ")\n",
    "\n",
    "# Create a data loader for the validation dataset\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,    # The validation dataset\n",
    "    batch_size=16,  # Batch size for each iteration\n",
    "    shuffle=False,  # Do not shuffle the data during validation\n",
    "    drop_last=False  # Do not drop the last batch during validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a UNet++ model with 1 output channel (for binary segmentation) and 3 input channels (for RGB images)\n",
    "model = UNetPP(1, 3, True)\n",
    "\n",
    "# Check if a GPU is available and move the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Define the loss function for binary segmentation (BCEWithLogitsLoss)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Filter model parameters that require gradients for optimization\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "# Define the optimizer (Adam) with a learning rate of 1e-3 and weight decay of 1e-4\n",
    "optimizer = optim.Adam(params, lr=1e-3, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the train data loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the specified number of epochs\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch [{epoch}/{epochs}]')\n",
    "\n",
    "    # Train the model for one epoch and record training and validation metrics\n",
    "    train_log = train(True, train_loader, model, criterion, optimizer)\n",
    "    val_log = validate(True, val_loader, model, criterion)\n",
    "\n",
    "    # Print and log the training and validation metrics\n",
    "    print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n",
    "          % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n",
    "\n",
    "    # Update the log dictionary with epoch-specific metrics\n",
    "    log['epoch'].append(epoch)\n",
    "    log['loss'].append(train_log['loss'])\n",
    "    log['iou'].append(train_log['iou'])\n",
    "    log['val_loss'].append(val_log['loss'])\n",
    "    log['val_iou'].append(val_log['iou'])\n",
    "\n",
    "    # Save the log to a CSV file\n",
    "    pd.DataFrame(log).to_csv(log_path, index=False)\n",
    "\n",
    "    # Increment the trigger variable\n",
    "    trigger += 1\n",
    "\n",
    "    # Check if the validation IoU score is better than the best IoU score seen so far\n",
    "    if val_log['iou'] > best_iou:\n",
    "        # Save the model's state dictionary\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        best_iou = val_log['iou']\n",
    "        print(\"=> saved best model\")\n",
    "        trigger = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from predict import image_loader\n",
    "from source.network import UNetPP\n",
    "from argparse import ArgumentParser\n",
    "from albumentations.augmentations import transforms\n",
    "from albumentations.core.composition import Compose\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation pipeline for validation images\n",
    "val_transform = Compose([\n",
    "    transforms.Resize(256, 256),  # Resize images to a fixed size of 256x256 pixels\n",
    "    transforms.Normalize(),  # Normalize pixel values of the images\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_width = config[\"im_width\"]\n",
    "im_height = config[\"im_height\"]\n",
    "model_path = config[\"model_path\"]\n",
    "output_path = config[\"output_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a UNet++ model object\n",
    "model = UNetPP(1, 3, True)\n",
    "\n",
    "# Load pre-trained weights from the specified model path\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Check if a GPU is available and move the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Set the model's mode to evaluation\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = \"input/PNG/Original/115.png\"\n",
    "\n",
    "# Load and preprocess the test image using the image_loader function\n",
    "image = image_loader(test_img)\n",
    "\n",
    "# Convert the image to a batch of 1 image by adding an additional dimension\n",
    "image = np.expand_dims(image, 0)\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "image = torch.from_numpy(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU (CUDA) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Transfer the preprocessed image to the GPU\n",
    "    image = image.to(device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mask by passing the input image through the model\n",
    "mask = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the last output channel (assuming it's a multi-channel output)\n",
    "mask = mask[-1]\n",
    "\n",
    "# Convert the torch tensor to a numpy array\n",
    "mask = mask.detach().cpu().numpy()\n",
    "\n",
    "# Remove single-dimensional entries and convert output to a 2D array\n",
    "mask = np.squeeze(np.squeeze(mask, axis=0), axis=0)\n",
    "\n",
    "# Convert the output to binary based on a threshold\n",
    "mask[mask > -2.5] = 255\n",
    "mask[mask <= -2.5] = 0\n",
    "\n",
    "# Resize the output mask to the input image size (im_width, im_height)\n",
    "mask = cv2.resize(mask, (im_width, im_height))\n",
    "\n",
    "plt.imshow(mask, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and plot the ground truth mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the ground truth mask image\n",
    "actual_mask = \"input/PNG/Ground Truth/115.png\"\n",
    "\n",
    "# Load the ground truth mask image using plt.imread\n",
    "am = plt.imread(actual_mask)\n",
    "\n",
    "plt.imshow(am, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UNET++.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
